{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n# Agents Lab Notebook v1.0.0\nThis notebook contains steps and code to demonstrate the use of agents\nconfigured in Agent Lab in watsonx.ai. It introduces Python API commands\nfor authentication using API key and invoking a LangGraph agent with a watsonx chat model.\n\n**Note:** Notebook code generated using Agent Lab will execute successfully.\nIf code is modified or reordered, there is no guarantee it will successfully execute.\nFor details, see: <a href=\"/docs/content/wsj/analyze-data/fm-prompt-save.html?context=wx\" target=\"_blank\">Saving your work in Agent Lab as a notebook.</a>\n\nSome familiarity with Python is helpful. This notebook uses Python 3.11.\n\n## Notebook goals\nThe learning goals of this notebook are:\n\n* Defining a Python function for obtaining credentials from the IBM Cloud personal API key\n* Creating an agent with a set of tools using a specified model and parameters\n* Invoking the agent to generate a response \n\n# Setup"}, {"metadata": {}, "cell_type": "code", "source": "# import dependencies\nfrom langchain_ibm import ChatWatsonx\nfrom ibm_watsonx_ai import APIClient\nfrom langchain_core.messages import AIMessage, HumanMessage\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.prebuilt import create_react_agent\nfrom ibm_watsonx_ai.foundation_models.utils import Tool, Toolkit\nimport json\nimport requests", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## watsonx API connection\nThis cell defines the credentials required to work with watsonx API for Foundation\nModel inferencing.\n\n**Action:** Provide the IBM Cloud personal API key. For details, see\n<a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\" target=\"_blank\">documentation</a>.\n"}, {"metadata": {}, "cell_type": "code", "source": "import os\nimport getpass\n\ndef get_credentials():\n\treturn {\n\t\t\"url\" : \"https://us-south.ml.cloud.ibm.com\",\n\t\t\"apikey\" : getpass.getpass(\"Please enter your api key (hit enter): \")\n\t}\n\ndef get_bearer_token():\n    url = \"https://iam.cloud.ibm.com/identity/token\"\n    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n    data = f\"grant_type=urn:ibm:params:oauth:grant-type:apikey&apikey={credentials['apikey']}\"\n\n    response = requests.post(url, headers=headers, data=data)\n    return response.json().get(\"access_token\")\n\ncredentials = get_credentials()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Using the agent\nThese cells demonstrate how to create and invoke the agent\nwith the selected models, tools, and parameters.\n\n## Defining the model id\nWe need to specify model id that will be used for inferencing:"}, {"metadata": {}, "cell_type": "code", "source": "model_id = \"ibm/granite-3-3-8b-instruct\"", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Defining the model parameters\nWe need to provide a set of model parameters that will influence the\nresult:"}, {"metadata": {}, "cell_type": "code", "source": "parameters = {\n    \"frequency_penalty\": 0,\n    \"max_tokens\": 2000,\n    \"presence_penalty\": 0,\n    \"temperature\": 0,\n    \"top_p\": 1\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Defining the project id or space id\nThe API requires project id or space id that provides the context for the call. We will obtain\nthe id from the project or space in which this notebook runs:"}, {"metadata": {}, "cell_type": "code", "source": "project_id = os.getenv(\"PROJECT_ID\")\nspace_id = os.getenv(\"SPACE_ID\")\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Creating the agent\nWe need to create the agent using the properties we defined so far:"}, {"metadata": {}, "cell_type": "code", "source": "client = APIClient(credentials=credentials, project_id=project_id, space_id=space_id)\n\n# Create the chat model\ndef create_chat_model():\n    chat_model = ChatWatsonx(\n        model_id=model_id,\n        url=credentials[\"url\"],\n        space_id=space_id,\n        project_id=project_id,\n        params=parameters,\n        watsonx_client=client,\n    )\n    return chat_model", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from ibm_watsonx_ai.deployments import RuntimeContext\n\ncontext = RuntimeContext(api_client=client)\n\n\n\n\ndef create_utility_agent_tool(tool_name, params, api_client, **kwargs):\n    from langchain_core.tools import StructuredTool\n    utility_agent_tool = Toolkit(\n        api_client=api_client\n    ).get_tool(tool_name)\n\n    tool_description = utility_agent_tool.get(\"description\")\n\n    if (kwargs.get(\"tool_description\")):\n        tool_description = kwargs.get(\"tool_description\")\n    elif (utility_agent_tool.get(\"agent_description\")):\n        tool_description = utility_agent_tool.get(\"agent_description\")\n    \n    tool_schema = utility_agent_tool.get(\"input_schema\")\n    if (tool_schema == None):\n        tool_schema = {\n            \"type\": \"object\",\n            \"additionalProperties\": False,\n            \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n            \"properties\": {\n                \"input\": {\n                    \"description\": \"input for the tool\",\n                    \"type\": \"string\"\n                }\n            }\n        }\n    \n    def run_tool(**tool_input):\n        query = tool_input\n        if (utility_agent_tool.get(\"input_schema\") == None):\n            query = tool_input.get(\"input\")\n\n        results = utility_agent_tool.run(\n            input=query,\n            config=params\n        )\n        \n        return results.get(\"output\")\n    \n    return StructuredTool(\n        name=tool_name,\n        description = tool_description,\n        func=run_tool,\n        args_schema=tool_schema\n    )\n\n\ndef create_custom_tool(tool_name, tool_description, tool_code, tool_schema, tool_params):\n    from langchain_core.tools import StructuredTool\n    import ast\n\n    def call_tool(**kwargs):\n        tree = ast.parse(tool_code, mode=\"exec\")\n        custom_tool_functions = [ x for x in tree.body if isinstance(x, ast.FunctionDef) ]\n        function_name = custom_tool_functions[0].name\n        compiled_code = compile(tree, 'custom_tool', 'exec')\n        namespace = tool_params if tool_params else {}\n        exec(compiled_code, namespace)\n        return namespace[function_name](**kwargs)\n        \n    tool = StructuredTool(\n        name=tool_name,\n        description = tool_description,\n        func=call_tool,\n        args_schema=tool_schema\n    )\n    return tool\n\ndef create_custom_tools():\n    custom_tools = []\n\n\ndef create_tools(context):\n    tools = []\n    \n    config = {\n    }\n    tools.append(create_utility_agent_tool(\"WebCrawler\", config, client))\n    config = {\n    }\n    tools.append(create_utility_agent_tool(\"DuckDuckGo\", config, client))\n    config = {\n        \"maxResults\": 10\n    }\n    tools.append(create_utility_agent_tool(\"GoogleSearch\", config, client))\n    config = {\n        \"maxResults\": 5\n    }\n    tools.append(create_utility_agent_tool(\"Wikipedia\", config, client))\n\n    return tools", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def create_agent(context):\n    # Initialize the agent\n    chat_model = create_chat_model()\n    tools = create_tools(context)\n\n    memory = MemorySaver()\n    instructions = \"\"\"Always respond clearly and professionally using structured, beginner-friendly language.\n\nIf a tool is required to answer a query (e.g., vector search for resume), respond with a <|tool_call|> followed by the appropriate JSON.\n\nIf a resume is available in the vector index, you must reference it in your responses.\n\nUse vector search queries such as:\n\n\\\"Extract job title from resume\\\"\n\n\\\"Get technical and soft skills from resume\\\"\n\n\\\"Summarize candidate's experience from resume\\\"\n\nDo not say \u201cI can\u2019t access the resume\u201d unless the vector tool is truly unavailable or empty.\n\nUse structured lists (bullet points or numbered steps) for clarity in questions, feedback, or strategies.\n\nIf the resume is not available, ask for the user\u2019s job title and experience level before continuing.\n\nDo not fabricate or assume resume content \u2014 always retrieve from the vector index or ask the user.\n\nRemain encouraging, helpful, and professional at all times.\n\nFocus on building the user's confidence while offering honest, constructive feedback.\n\nNever suggest irrelevant, unrealistic, or outdated interview questions.\n\n\nYou are an intelligent and professional interview preparation assistant. Your role is to simulate a job candidate by directly answering mock interview questions based on the provided job title and experience level.\n\n\u2705 What You Do:\nGenerate 10\u201315 realistic interview questions at a time, covering:\n\nTechnical/domain-specific questions\n\nBehavioral/HR questions\n\nRole-specific challenges or situational problems\n\nFor each question, immediately provide a generalized sample answer, as though you are the candidate.\n\n\ud83d\udeab What You Do NOT Do:\nDo not ask the user for a resume or any additional information.\nDo not ask follow-up questions or wait for user responses.\n\nDo not prompt the user to answer \u2014 you answer all questions yourself.\n\n\u270d\ufe0f Guidelines for the Answers:\nTailor answers based on the job role (e.g., Python Developer, Data Analyst) and experience level (e.g., Fresher, 2\u20133 years).\n\nKeep responses concise, structured, and professional.\n\nUse bullet points or clear paragraphs when helpful.\n\nUse confident, realistic language \u2014 not exaggerated or robotic.\n\nKeep the tone supportive and interview-appropriate.\n\nFormat Example:\n\nInterview Questions & Answers \u2013 Python Developer (2 Years Experience)\nQ1. What Python libraries have you used for data analysis?\nA1.\nI\u2019ve primarily worked with:\n\nPandas for data manipulation and cleaning\n\nNumPy for numerical computations\n\nMatplotlib and Seaborn for visualization\n\nI\u2019ve also used Scikit-learn for basic machine learning tasks\nThese tools have been part of my workflow in automating reporting and generating insights from structured data.\n\nQ2. Describe a challenging bug you solved.\nA2.\nIn one project, a data processing pipeline was silently dropping rows. After debugging, I traced it to a faulty condition in a lambda filter that excluded rows with nulls, even when the column was optional. I corrected the logic, added test cases, and also implemented logging to catch such silent failures in the future.\"\"\"\n\n    agent = create_react_agent(chat_model, tools=tools, checkpointer=memory, state_modifier=instructions)\n\n    return agent", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Visualize the graph\nfrom IPython.display import Image, display\nfrom langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n\nImage(\n    create_agent(context).get_graph().draw_mermaid_png(\n        draw_method=MermaidDrawMethod.API,\n    )\n)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Invoking the agent\nLet us now use the created agent, pair it with the input, and generate the response to your question:\n"}, {"metadata": {}, "cell_type": "code", "source": "agent = create_agent(context)\n\ndef convert_messages(messages):\n    converted_messages = []\n    for message in messages:\n        if (message[\"role\"] == \"user\"):\n            converted_messages.append(HumanMessage(content=message[\"content\"]))\n        elif (message[\"role\"] == \"assistant\"):\n            converted_messages.append(AIMessage(content=message[\"content\"]))\n    return converted_messages\n\nquestion = input(\"Question: \")\n\nmessages = [{\n    \"role\": \"user\",\n    \"content\": question\n}]\n\ngenerated_response = agent.invoke(\n    { \"messages\": convert_messages(messages) },\n    { \"configurable\": { \"thread_id\": \"42\" } }\n)\n\nprint_full_response = False\n\nif (print_full_response):\n    print(generated_response)\nelse:\n    result = generated_response[\"messages\"][-1].content\n    print(f\"Agent: {result}\")\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Next steps\nYou successfully completed this notebook! You learned how to use\nwatsonx.ai inferencing SDK to generate response from the foundation model\nbased on the provided input, model id and model parameters. Check out the\nofficial watsonx.ai site for more samples, tutorials, documentation, how-tos, and blog posts.\n\n<a id=\"copyrights\"></a>\n### Copyrights\n\nLicensed Materials - Copyright \u00a9 2024 IBM. This notebook and its source code are released under the terms of the ILAN License.\nUse, duplication disclosure restricted by GSA ADP Schedule Contract with IBM Corp.\n\n**Note:** The auto-generated notebooks are subject to the International License Agreement for Non-Warranted Programs (or equivalent) and License Information document for watsonx.ai Auto-generated Notebook (License Terms), such agreements located in the link below. Specifically, the Source Components and Sample Materials clause included in the License Information document for watsonx.ai Studio Auto-generated Notebook applies to the auto-generated notebooks.  \n\nBy downloading, copying, accessing, or otherwise using the materials, you agree to the <a href=\"https://www14.software.ibm.com/cgi-bin/weblap/lap.pl?li_formnum=L-AMCU-BYC7LF\" target=\"_blank\">License Terms</a>  "}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}}, "nbformat": 4, "nbformat_minor": 0}